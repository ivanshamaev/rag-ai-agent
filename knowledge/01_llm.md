# Что такое большие языковые модели (LLM)

В этом разделе — определение LLM, ключевые принципы и архитектура трансформера.

## Определение

Большая языковая модель (LLM) — нейросеть, обученная на больших объёмах текстовых данных для предсказания следующего токена или вероятностного моделирования текста. LLM используются для генерации, понимания, суммаризации и трансформации текста.

## Архитектура трансформера

- **Self-Attention**: ключевой механизм, позволяющий каждому токену учитывать все остальные токены в контексте.
- **Q/K/V**: Query, Key, Value — проекции входов, между которыми считается внимание.
 - **Multi-Head Attention**: несколько параллельных «голов» внимания, каждая обучается выделять разные типы зависимостей.
 - **Feed-Forward**: позиционно-независимая двухслойная MLP после слоя внимания.
 - **LayerNorm и Residuals**: нормализации и остаточные связи для стабильности обучения.

## Параметры и масштабирование

- Размер модели (кол-во параметров), глубина (число слоев) и ширина слоёв влияют на выраженные способности.
- Scaling laws: увеличение данных и параметров даёт предсказуемое улучшение качества, но требует вычислительных ресурсов.

## Токенизация

- BPE, SentencePiece и WordPiece — стандартные методы сегментации текста на субсловные токены. Выбор токенизатора влияет на эффективность представления редких слов и иностранных языков.

## Обучение и дообучение

- **Предобучение**: языковое моделирование на большом корпусе.
- **Дообучение** (fine-tuning): адаптация к конкретной задаче (например, классификация, ответ на вопросы).
- **Instruction tuning / RLHF**: методики для улучшения поведения модели в интерактивных сценариях.

## Ограничения и риски

- **Галлюцинации**: генерация неправдоподобной или ложной информации.
- **Биас**: предвзятость, унаследованная из обучающих данных.
- **Приватность**: возможное запоминание чувствительной информации.

## Ресурсы для изучения

- "Attention Is All You Need" — Vaswani et al.
- Документация и статьи по GPT, BERT, Transformer-XL и др.

## Пояснение для новичка: основные термины

- **Токен**: минимальная единица текста, с которой работает модель (буква, часть слова или слово). Токенизация переводит текст в последовательность токенов.
- **Параметры модели**: числа (веса) внутри нейросети; чем их больше, тем больше модель "памяти" и выразительности, но и требований к ресурсам.
- **Контекстное окно**: максимальное число токенов, которое модель может учитывать за один раз.
- **Fine-tuning (дообучение)**: процесс дополнительного обучения модели на специфичных данных для улучшения поведения в конкретной задаче.

Если вы новичок, начните с понимания токенов и простых примеров prompt'ов: попробуйте отправить короткий вопрос в открытую LLM и посмотреть, как модель отвечает, затем постепенно углубляйтесь в архитектуру.
