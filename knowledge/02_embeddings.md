## Эмбеддинги

**Эмбеддинги** — это плотные векторные представления объектов (слов, предложений, документов, изображений), которые сохраняют их **семантическое значение**. Математически — это способ перевести смысл объекта в набор чисел фиксированной длины так, чтобы похожие объекты располагались ближе друг к другу в векторном пространстве.

---

## Что такое эмбеддинги

Эмбеддинг — это **вектор фиксированной размерности**, вычисленный моделью.  
Если два объекта похожи по смыслу, их векторы будут **близки** друг к другу; если разные — будут располагаться далеко.

Пример:  
- «кофе» и «капучино» → близкие векторы  
- «кофе» и «автомобиль» → далёкие векторы  

---

## Как получают эмбеддинги

### 1. Прямое обучение
Модели обучаются предсказывать контекст или пропущенные токены.  
Классические методы:
- **Word2Vec** (CBOW / Skip-gram)
- **FastText** (учитывает подслова)
  
Такие модели учатся на статистике совместных появлений слов.

### 2. Трансформеры
Современные эмбеддинги получают из encoder-части трансформеров или pooled-выхода моделей.

Примеры:
- **BERT**, **RoBERTa** — encoder models  
- **Sentence-Transformers (SBERT)** — специализированные модели для получения семантических эмбеддингов  
- decoder-only модели (LLaMA, GPT) также могут выдавать эмбеддинги через pooling скрытых состояний

### 3. Контрастивное обучение (Siamese-сетап)
Используются пары «похожих» и «не похожих» текстов.

Подходы:
- **Siamese networks**
- **SBERT**
- Contrastive learning  

Цель — сделать семантически близкие тексты ближе в пространстве.

---

## Типы эмбеддингов

### Текстовые
Эмбеддинги:
- слов
- предложений
- абзацев
- документов

Используются в поиске, кластеризации, RAG, классификации.

### Семантические
Спроектированы специально для того, чтобы семантически близкие фразы имели минимальную дистанцию между векторами.

Используются в:
- semantic search  
- reranking  
- question-answering  

### Мультимодальные
Единое пространство для изображений и текста.

Примеры:
- **CLIP** (текст + изображение)
- ALIGN
- LiT

Используются в визуальном поиске и генерации.

---

## Сравнение эмбеддингов

### Размерность
- Низкая (128–384) → быстро, подходит для real-time retrieval  
- Средняя (512–768) → баланс качества и скорости  
- Высокая (1024–1536+) → больше качества, больше latency и памяти  

### Качество
Зависит от:
- архитектуры модели  
- объёма данных обучения  
- доменной специфики  

### Метрика близости
- **Косинусное сходство** — стандарт для семантики  
- **Евклидово расстояние** — лучше для некоторых ANN-индексов  
- **Dot product** — используется в FAISS / ScaNN для максимальной скорости

---

## Практические советы

- **Нормализуйте векторы** (L2 norm), если используете косинусное сходство.  
- **Выбирайте размерность**, исходя из trade-off `качество vs латентность`.  
  - Вектор 1536 даёт более точную семантику, но тяжелее в продакшене.  
- **Файнтюнинг на доменной разметке** (Q&A пары, похожие документы) значительно повышает релевантность в RAG-системах.
- Для миллиардов векторов используйте ANN (HNSW, IVF-PQ, ScaNN).

---

## Пояснение для новичка: простые термины

### Вектор
Список чисел:
[0.12, -0.03, 0.98, ...]


Он выражает смысл объекта в числовой форме.

### Косинусное сходство
Показывает, насколько два вектора похожи по направлению.  
1 — максимально похожие;  
0 — никак не связаны;  
-1 — противоположные.

### ANN (Approximate Nearest Neighbor)
Алгоритмы быстрого поиска похожих векторов в больших коллекциях:  
FAISS, HNSW, ScaNN, Milvus, Qdrant.

### Простой пример
Фразы:

- «Как приготовить кофе»  
- «Рецепт кофе»

будут иметь почти одинаковые эмбеддинги → система поиска вернёт их как похожие, даже если слова не совпадают.
