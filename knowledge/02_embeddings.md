# Эмбеддинги

Эмбеддинги — плотные векторные представления объектов (слов, предложений, документов), сохраняющие семантическую информацию.

## Что такое эмбеддинги

- Вектор фиксированной размерности, вычисляемый моделью, который помещает похожие по смыслу элементы ближе в пространстве.

## Как их получают

- **Прямое обучение**: модели обучаются предсказывать контекст/маскированные токены (например, Word2Vec, FastText).
- **Трансформеры**: современные эмбеддинги генерируются через encoder (BERT) или pooled output decoder-only моделей (sentence-transformers).
- **Сшимментирование**: методики типа contrastive learning (Siamese, SBERT) для обучения семантических эмбеддингов.

## Типы эмбеддингов

- **Текстовые**: слова, предложения, абзацы, документы.
- **Семантические**: спроектированы так, чтобы семантически похожие тексты имели близкие векторы.
- **Мультимодальные**: эмбеддинги изображений и текста в одном пространстве (CLIP и др.).

## Сравнение эмбеддингов

- **Размерность**: trade-off между выразительностью и скоростью поиска.
- **Качество**: зависит от архитектуры и объёма данных.
- **Выбор метрики**: косинусное сходство часто используется для семантики, евклидово расстояние — для некоторых VDB.

## Практические советы

- Нормализуйте векторы перед поиском (L2-нормирование) при использовании косинусного сходства.
- Подбирайте размерность под качество и латентность (128–1536 — типичные значения).
- Файнтюнинг эмбеддингов на доменной разметке улучшает релевантность.

## Пояснение для новичка: термины и простые примеры

- **Вектор**: список чисел, например [0.12, -0.03, 0.98]. Эмбеддинги представляют слова/фразы как такие списки.
- **Косинусное сходство**: число от -1 до 1, которое показывает, насколько два вектора похожи по направлению; чем ближе к 1 — тем ближе по смыслу.
- **ANN (Approximate Nearest Neighbor)**: алгоритм для быстрого поиска похожих векторов, потому что точный поиск медленный на больших наборах.

Пример: предложение "Как приготовить кофе" и "Рецепт кофе" будут иметь похожие эмбеддинги, поэтому поиск по эмбеддингам вернёт эти фразы как семантически похожие.
