# Архитектура RAG-систем

Retrieval-Augmented Generation (RAG) сочетает генеративные модели с retrieval, чтобы сделать ответы более фактологичными и основанными на внешних данных.

## Компоненты RAG

- **Хранилище документов**: коллекция источников (файлы, БД, веб-контент).
- **Indexer / Embedding store**: векторная БД или инвертированный индекс для поиска релевантных фрагментов.
- **Retriever**: модуль, выполняющий поиск по запросу.
- **Reranker**: (опционально) модель, уточняющая порядок кандидатов.
- **Generator**: LLM, который генерирует финальный ответ, используя retrieved контекст.
- **Контекстный менеджер**: логика для составления prompt'а и управления длиной контекста (chunk selection, prompt engineering).

## Варианты интеграции

- **Fusion-in-Decoder**: генератор получает множество retrieved кусков и аггрегирует в декодере.
- **Retrieval-then-Generate**: retrieve → concatenate → generate (простая и распространённая схема).

## Особенности проектирования

- Управление контекстной длиной: выбирать оптимальное количество фрагментов и длину чанков.
- Инкрементальное обновление индексов: поддерживать актуальность хранилища.
- Трассировка источников (source attribution): включать ссылки на источники в ответах.

## Пояснение для новичка: ключевые понятия

- **RAG (Retrieval-Augmented Generation)**: идея — не заставлять саму LLM хранить все факты, а подставлять свежие фрагменты из внешнего хранилища.
- **Chunk (чанк)**: небольшой фрагмент текста (параграф или несколько предложений), который хранится отдельно в индексе.
- **Prompt engineering**: искусство составления запроса к модели, чтобы она дала нужный ответ (например: вложить retrieved фрагменты и явно попросить ссылаться на источник).

Для новичка: представьте, что LLM — это писатель, а retriever — библиотекарь, который приносит нужные книги. RAG объединяет их: библиотекарь даёт материалы, а писатель пишет ответ, опираясь на них.
