# Пайплайн RAG: ingestion → chunking → embedding → retrieval → генерация

Пошаговый разбор типичного пайплайна RAG.

## 1) Ingestion (загрузка данных)

- Источники: документы, базы данных, веб-страницы, API.
- Очистка: удаление шума, нормализация кодировок, извлечение текста.

## 2) Chunking (разбиение)

- Разбивать документы на логичные фрагменты (параграфы, предложения) или на фиксированные window.
- Поддерживать overlap (перекрытие) при необходимости для сохранения контекста.

## 3) Embedding

- Для каждого чанка вычисляются эмбеддинги с помощью sentence-transformers или LLM-encoder.
- Нормализация векторов и приведение к единому формату.

## 4) Indexing & Retrieval

- Загрузка векторов в векторную БД (Milvus, FAISS, Qdrant) или построение inverted index.
- Настройка ANN, выбор метрики (cosine, inner product, euclidean).

## 5) Generация

- Составление prompt: включение top-k retrieved фрагментов, системных инструкций и вопроса.
- Генерация ответа моделью — decoder-only LLM или encoder-decoder.

## Дополнительные шаги

- **Reranking**: дополнительные модели для оценки relevance.
- **Пост-обработка**: валидация фактов, добавление источников, отбрасывание небезопасного контента.

## Пояснение для новичка: шаги простыми словами

- **Ingestion**: собрать все тексты, которые вы хотите использовать (например, документацию или статьи).
- **Chunking**: разбить большие тексты на небольшие удобные для поиска куски — как разрезать книгу на главы и абзацы.
- **Embedding**: превратить каждый кусок в вектор (набор чисел), чтобы сравнивать по смыслу.
- **Indexing**: поместить векторы в базу, чтобы быстро находить похожие.
- **Retrieval + Generation**: по запросу найти несколько подходящих чанков и дать их LLM для составления ответа.

Простой сценарий: хотите ответить на вопрос по документации — ingestion загружает документацию, chunking разбивает её, embedding и indexing позволяют быстро найти нужные абзацы, затем LLM генерирует ответ на основе найденных абзацев.
