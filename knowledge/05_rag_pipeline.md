# Пайплайн RAG: ingestion → chunking → embedding → retrieval → генерация

Ниже представлен полный разбор стандартного RAG-пайплайна — от загрузки данных до генерации ответа. Этот процесс лежит в основе большинства продакшн-систем, использующих Retrieval-Augmented Generation.

---

## 1) Ingestion (загрузка данных)

Первый этап — собрать и подготовить сырой контент:

- **Источники данных**: документы (PDF, DOCX, TXT), базы данных, корпоративные вики, веб-страницы, API, логи.  
- **Очистка**:  
  - удаление HTML-тегов, скриптов, служебных символов  
  - нормализация кодировок  
  - извлечение текста из PDF/сканов  
  - дедупликация  
- **Структурирование**: выделение разделов, заголовков, метаданных.

Цель ingestion — создать чистый, пригодный для разбиения набор текстов.

---

## 2) Chunking (разбиение документа на фрагменты)

Большие документы превращаются в набор маленьких элементов (чанков):

- Разбиение по логической структуре: главы, параграфы, списки.  
- Разбиение по окнам (например, каждые 300–500 токенов).  
- Использование **overlap (перекрытия)** между чанками (20–30%), чтобы сохранить контекст между фрагментами.  
- Добавление метаданных: путь файла, заголовок, дата, автор, typetag.

Chunking оптимизирует работу retriever’а: маленькие фрагменты ищутся точнее и вставляются в prompt без переполнения контекста.

---

## 3) Embedding (векторизация чанков)

Каждый чанк преобразуется в вектор:

- Используются модели типа **sentence-transformers**, **E5**, **bge**, **miniLM**, encoder-части LLM или API-энкодеры.  
- Генерация плотных векторов фиксированной размерности (например, 768 или 1024).  
- **Нормализация векторов** (L2-normalization) — важна для корректного сравнения, особенно при cosine similarity.  
- Сохранение: вектор + текст + метаданные.

На этом этапе контент становится пригодным для быстрого поиска по смыслу.

---

## 4) Indexing & Retrieval

### Indexing
- Загрузка векторов в векторную базу: **Milvus, Qdrant, Weaviate, Chroma** или **FAISS**.  
- Оптимизация индекса:  
  - выбор структуры (HNSW, IVF, Flat, PQ)  
  - параметров поиска (efSearch, nlist, nprobe)  
- Альтернатива или дополнение — sparse индекс (BM25).

### Retrieval
- Поиск по запросу пользователя:  
  - dense (по векторному сходству)  
  - sparse (по словам)  
  - hybrid (fusion sparse+dense)  
- Выбор метрики: **cosine**, **inner product**, **euclidean**.

Retriever возвращает top-k наиболее релевантных чанков, которые будут использоваться в prompt’e.

---

## 5) Генерация ответа (Generation)

- Сборка prompt:  
  - системные инструкции  
  - retrieved чанки (top-k)  
  - пользовательский запрос  
- Отправка собранного prompt’a в LLM:  
  - модели типа GPT, LLaMA, Mistral, Qwen, Claude  
  - обычно decoder-only LLM  
- Получение итогового ответа, который опирается на retrieved информацию.

Этот шаг превращает сырые найденные данные в структурированный, человекопонятный ответ.

---

## Дополнительные шаги

### Reranking
Дополнительная фильтрация retrieved чанков:
- cross-encoder модели (например, monoT5, ColBERT)  
- LLM-based scoring  
- повышает качество, но добавляет latency

### Пост-обработка ответа
- валидация фактов (fact-checking)  
- добавление ссылок на источники  
- фильтрация PII  
- безопасные трансформации контента  
- форматирование (markdown, списки, таблицы)

---

## Пояснение для новичка: пайплайн простыми словами

- **Ingestion** — собрать тексты, очистить, подготовить.  
- **Chunking** — нарезать их на удобные маленькие куски.  
- **Embedding** — превратить каждый кусок в вектор, чтобы можно было сравнивать по смыслу.  
- **Indexing** — сохранить векторы в базу, которая умеет быстро искать похожие.  
- **Retrieval + Generation** — найти подходящие фрагменты и дать их модели, чтобы она написала правильный ответ.

### Пример в жизни
Вы хотите ответить на вопрос по документации продукта:
1. Документация загружается (ingestion).  
2. Разрезается на абзацы (chunking).  
3. Каждый абзац превращается в вектор (embedding).  
4. Эти векторы помещаются в индекс (indexing).  
5. Когда пользователь задаёт вопрос — ищем подходящие абзацы, а LLM пишет ответ (retrieval + generation).

---

Если нужно — могу добавить архитектурную диаграмму, последовательность в виде схемы или пример пайплайна на Python (LangChain / LlamaIndex / custom).  
