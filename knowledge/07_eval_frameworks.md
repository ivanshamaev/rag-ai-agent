# Eval-фреймворки и метрики для RAG

Оценка RAG-систем (Retrieval-Augmented Generation) требует анализа двух независимых частей пайплайна:  
1) **retrieval (поиска фрагментов)** и  
2) **generation (качества сгенерированного ответа)**.  

Хорошая система должна одновременно:  
- находить релевантные фрагменты,  
- правильно опираться на них,  
- давать фактологически корректные и понятные ответы.

---

## Ключевые метрики

### 1) Метрики fact-grounding (основанности на источниках)
Проверяют, ссылается ли ответ на retrieved контекст и не «галлюцинирует» ли модель.

- **Faithfulness** — насколько утверждения ответа соответствуют найденным фрагментам.  
- **Groundedness** — доля ответа, которую можно подтвердить источниками.

Обычно вычисляется LLM-оценкой или библиотеками (например, RAGAS).

---

### 2) Retrieval-метрики

Используются для оценки качества retriever’ов (dense, sparse или гибридных):

- **Recall@k** — среди всех релевантных фрагментов, сколько попало в top-k.  
  *Показывает полноту поиска; важно для RAG, чтобы не пропустить нужный контент.*  

- **Precision@k** — среди top-k найденных, какая доля релевантных.  
  *Показывает точность и качество сортировки кандидатов.*

- **MRR (Mean Reciprocal Rank)** — насколько высоко retriever помещает первый релевантный документ.

---

### 3) Текстовые метрики для generation

Стандартные NLP-метрики, полезны, но **не отражают фактологичность**:

- **ROUGE (1/2/L)** — для измерения перекрытия n-грамм (актуально в суммаризации).  
- **BLEU** — сравнение n-грамм с эталоном (в QA/переводе).  
- **METEOR** — улучшенная тестовая метрика (синонимы + стемминг).

Эти метрики дают общую картину качества текста, но должны дополняться fact-checking метриками.

---

### 4) Exact Match (EM) и F1

Используются в **extractive QA** или задачах с чётким эталонным ответом:

- **EM** — точное совпадение ответа с эталоном.  
- **F1** — учитывает частичное совпадение (пересечение токенов).  

Для RAG применимы в доменах с чёткой структурой ответов (медицина, поддержка, документация).

---

### 5) Human Evaluation

Когда автоматические метрики не хватает, применяется ручная оценка:

- проверка фактологичности  
- полезности  
- полноты  
- стиля  
- безопасности  
- отсутствия токсичности и галлюцинаций  

Часто используется в продакшене как основная метрика качества.

---

## Eval-фреймворки

### **RAGAS**
Специализированный набор метрик для комплексной оценки RAG-систем:
- оценка retrieved фрагментов  
- faithfulness  
- answer relevancy  
- context precision / recall  
- answer groundedness  

RAGAS умеет автоматически генерировать отчёты, использовать LLM для оценки, проводить пайплайн-тесты.

---

### **TruLens**
Фреймворк для анализа LLM-пайплайнов:
- интерпретация поведения модели  
- оценка правдоподобности  
- проверка зависимости ответа от контекста  
- мониторинг LLM в продакшене  

Полезен для аудита RAG и поиска слабых мест.

---

### **Hugging Face Evaluate / Datasets**
Набор готовых вычислителей стандартных метрик:
- ROUGE, BLEU, METEOR  
- Precision/Recall/MRR  
- EM, F1  
- Поддержка кастомных датасетов для тестирования retrieval и generation.

Отличный инструмент для единообразной автоматизации экспериментов.

---

## Практики измерения качества RAG

- Используйте **комбинацию retrieval-метрик и generation-метрик**, иначе вы измеряете только половину системы.  
- Обязательно включайте **метрики groundedness**, чтобы контролировать галлюцинации.  
- Дополняйте автоматические метрики **human evaluation**, особенно в важных доменах.  
- Разделяйте тесты на:
  - **retriever-only eval** (Recall@k)  
  - **end-to-end eval (RAG)** — faithfulness, relevancy, human judgements  
- Тестируйте на adversarial примерах: перефразирования, длинные запросы, конфликтующие данные.  
- Проверяйте **качество источников** (source attribution) и способность модели ссылаться на retrieved документы.

---

## Пояснение для новичка: как понять, хороша ли RAG-система

Чтобы понять, работает ли RAG:

1. **Она должна находить правильные фрагменты**  
   — это измеряют Recall@k и Precision@k.

2. **Она должна писать ответы, опираясь на найденные данные**  
   — это измеряют Faithfulness и Groundedness.

3. **Она должна быть полезной человеку**  
   — это оценивают люди (Human Eval).

### Простой подход для новичка:
- Посчитайте Recall@k для retriever’a → поймёте, умеет ли он находить нужные документы.  
- Используйте RAGAS или LLM-оценки для faithfulness → поймёте, опирается ли LLM на данные или придумывает.  
- Проведите human evaluation на небольшом наборе документов → получите реальное качество.

Как итог:  
**автоматика даёт скорость, люди — точность. Оценивать RAG нужно и теми, и другими методами.**
