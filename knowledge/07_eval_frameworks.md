# Eval-фреймворки и метрики для RAG

Оценка качества в RAG-системах включает измерения релевантности retrieved фрагментов и качества сгенерированных ответов.

## Ключевые метрики

- **Faithfulness / Groundedness**: доля утверждений, подтвержаемых источниками.
- **Precision / Recall @k**: для retrieval-части.
- **ROUGE / BLEU / METEOR**: общие текстовые метрики (не отражают фактичность).
- **EM (Exact Match)** и **F1**: для QA задач с эталонными ответами.
- **Human Eval**: разметка людьми для оценки качества и factuality.

## Eval-фреймворки

- **RAGAS**: набор методик и метрик для оценки RAG-систем (retrieval + generation).
- **TruLens**: фреймворк для интерпретации и анализа поведения LLM.
- **Hugging Face Datasets / Evaluate**: инструменты для расчёта стандартных метрик.

## Практики измерения

- Смешивайте автоматические метрики и human-in-the-loop оценки для factuality.

## Пояснение для новичка: как понять, хороша ли система

- **Faithfulness / Groundedness**: проверяем, действительно ли утверждения в ответе подтверждаются найденными источниками.
- **Precision@k и Recall@k**: измеряют, насколько среди top-k найденных фрагментов есть релевантные; чем выше — тем лучше retrieval.
- **Human Eval**: люди читают ответы и оценивают качество, фактическую корректность и понятность.

Простой подход к оценке: автоматические метрики дают быстрый обзор, но для реальных задач важно иметь выборку проверок людьми, особенно для чувствительных предметных областей.
- Оценивайте качество источников (source attribution) и устойчивость модели к adversarial prompts.
