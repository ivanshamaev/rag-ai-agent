# ETL-процессы: методологии и инструменты

ETL (Extract, Transform, Load) — ключ к стабильной подаче данных в ML-пайплайны.

## Методологии

- **Batch vs Streaming**: периодическая обработка больших фрагментов против обработки событий в реальном времени.
- **Idempotency**: важность идемпотентных задач для повторного запуска.
- **Data contracts**: соглашения о формате и валидации данных между командами.

## Инструменты

- **Apache Spark**: масштабируемая обработка данных, подходит для тяжелых трансформаций.
- **Airflow / Prefect**: планирование и оркестрация DAG'ов.
- **DBT**: трансформации в слое данных (analytics engineering).

## Best practices

- Логирование и метрики на каждом шаге ETL.
- Автоматические тесты данных (schema checks, null checks, ranges).
- Версионирование данных и reproducibility (датасеты с версиями).

## Пояснение для новичка: что это значит на практике

- **Extract**: получить данные из источников (файлы, БД, API).
- **Transform**: очистить, нормализовать и подготовить данные (удалить HTML-теги, исправить кодировки, нормализовать даты).
- **Load**: загрузить очищенные данные в хранилище или индекс.

Начните с простого: напишите скрипт, который читает несколько файлов, очищает текст и сохраняет в одном формате (например, JSON). Затем автоматизируйте этот процесс через cron или простую задачу в Airflow.
